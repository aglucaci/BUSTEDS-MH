{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rough-alcohol",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "Create a CSV file for all the the analyses in the Empirical Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-employment",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from prettytable import PrettyTable\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from scipy.stats import wasserstein_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-steel",
   "metadata": {},
   "source": [
    "## Declares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "outer-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local on Windows 10 box\n",
    "WD = os.path.join(\"E:\\\\\", \"BUSTEDS-MH-develop\")\n",
    "\n",
    "tags = [\"Empirical_14_datasets\", \"Empirical_Unmasked_Selectome_v6\", \"Empirical_Enard\", \n",
    "        \"Empirical_mtDNA_Invertebrate\", \"Empirical_mtDNA_Vertebrate\", \n",
    "        \"Empirical_Selectome_v7_Euteleostomi_unmasked\", \"Empirical_Shultz\" ]\n",
    "\n",
    "ER_Threshold = 5\n",
    "\n",
    "FILES_WITH_ERRORS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-lindsay",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sealed-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    #print(\"# Reading:\", filename)\n",
    "    if os.stat(filename).st_size == 0: \n",
    "        #print(\"# -- Error -- file is empty:\", filename)\n",
    "        return []\n",
    "    #end if\n",
    "    \n",
    "    with open(filename, \"r\") as fh:\n",
    "        json_data = json.load(fh)\n",
    "    fh.close()\n",
    "    return json_data\n",
    "#end method\n",
    "\n",
    "# define function to calculate cv\n",
    "#cv = lambda x: np.std(x, ddof=1) / np.mean(x) * 100 \n",
    "cv = lambda x: np.std(x) / np.mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7a7e6",
   "metadata": {},
   "source": [
    "## Look over BUSTED[S]-MH Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d65cee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BUSTEDS_MH_results(BUSTEDS_MH_DIR_FILES, tag, FILES_WITH_ERRORS):\n",
    "    df_dict = {}\n",
    "    for item in tqdm(BUSTEDS_MH_DIR_FILES):\n",
    "        basename = os.path.basename(item).replace(\".BUSTEDS-MH.json\", \"\")\n",
    "        \n",
    "        # Read json file\n",
    "        json_data_BUSTEDS_MH = read_json(item) # Returns \"[]\" if file is empty\n",
    "\n",
    "        if json_data_BUSTEDS_MH == []: \n",
    "            FILES_WITH_ERRORS.append(item)\n",
    "            continue\n",
    "        #end if\n",
    "        \n",
    "        #print(\"# Data loaded\")\n",
    "        df_dict[basename] = {\"Method\": \"BUSTEDS-MH\"}\n",
    "        df_dict[basename].update({\"Sequences\": json_data_BUSTEDS_MH[\"input\"][\"number of sequences\"]})\n",
    "        df_dict[basename].update({\"Codons\": json_data_BUSTEDS_MH[\"input\"][\"number of sites\"]})\n",
    "        df_dict[basename].update({\"LRT p-value\": json_data_BUSTEDS_MH[\"test results\"][\"p-value\"]})\n",
    "\n",
    "        # cAIC\n",
    "        df_dict[basename].update({\"cAIC\": json_data_BUSTEDS_MH[\"fits\"][\"Unconstrained model\"][\"AIC-c\"]})\n",
    "\n",
    "        #Omegas and proportions\n",
    "        data = json_data_BUSTEDS_MH[\"fits\"][\"Unconstrained model\"][\"Rate Distributions\"][\"Test\"]\n",
    "        w1 = round(data[\"0\"][\"omega\"], 4)\n",
    "        p1 = round(data[\"0\"][\"proportion\"], 4)\n",
    "        w2 = round(data[\"1\"][\"omega\"], 4)\n",
    "        p2 = round(data[\"1\"][\"proportion\"], 4)\n",
    "        w3 = round(data[\"2\"][\"omega\"], 4)\n",
    "        p3 = round(data[\"2\"][\"proportion\"], 4)\n",
    "        df_dict[basename].update({\"w1\": w1, \"p1\": p1})\n",
    "        df_dict[basename].update({\"w2\": w2, \"p2\": p2})\n",
    "        df_dict[basename].update({\"w3\": w3, \"p3\": p3})\n",
    "\n",
    "        df_dict[basename].update({\"CV(omega)\": cv([w1, w2, w3])})\n",
    "\n",
    "        # SRV rates and proportions\n",
    "        data = json_data_BUSTEDS_MH[\"fits\"][\"Unconstrained model\"][\"Rate Distributions\"][\"Synonymous site-to-site rates\"]\n",
    "        s1 = round(data[\"0\"][\"rate\"], 4)\n",
    "        s_p1 = round(data[\"0\"][\"proportion\"], 4)\n",
    "        s2 = round(data[\"1\"][\"rate\"], 4)\n",
    "        s_p2 = round(data[\"1\"][\"proportion\"], 4)\n",
    "        s3 = round(data[\"2\"][\"rate\"], 4)\n",
    "        s_p3 = round(data[\"2\"][\"proportion\"], 4)\n",
    "        df_dict[basename].update({\"SRV1\": s1, \"SRV_p1\": s_p1})\n",
    "        df_dict[basename].update({\"SRV2\": s2, \"SRV_p2\": s_p2})\n",
    "        df_dict[basename].update({\"SRV3\": s3, \"SRV_p3\": s3})\n",
    "        df_dict[basename].update({\"CV(alpha)\": cv([s1, s2, s3])})\n",
    "\n",
    "        # DH rate, TH rate, TH_SI rate\n",
    "        df_dict[basename].update({\"DH_Rate\": float(json_data_BUSTEDS_MH[\"fits\"][\"Unconstrained model\"]\n",
    "                                  [\"rate at which 2 nucleotides are changed instantly within a single codon\"])})\n",
    "        df_dict[basename].update({\"TH_Rate\": float(json_data_BUSTEDS_MH[\"fits\"][\"Unconstrained model\"]\n",
    "                                  [\"rate at which 3 nucleotides are changed instantly within a single codon\"])})\n",
    "        df_dict[basename].update({\"TH_Rate_SI\": float(json_data_BUSTEDS_MH[\"fits\"][\"Unconstrained model\"]\n",
    "                                  [\"rate at which 3 nucleotides are changed instantly within a single codon between synonymous codon islands\"])})\n",
    "\n",
    "        # ER Sites, thresholded\n",
    "        ER_SITES = []\n",
    "        ER_df_dict = {}\n",
    "        if \"constrained\" in json_data_BUSTEDS_MH[\"Evidence Ratios\"].keys():\n",
    "            #print(\"# ER Constrained Sites:\", len(json_data_BUSTEDS_MH[\"Evidence Ratios\"][\"constrained\"][0]))\n",
    "            for site, val in enumerate(json_data_BUSTEDS_MH[\"Evidence Ratios\"][\"constrained\"][0]):\n",
    "                if val > ER_Threshold:\n",
    "                    ER_SITES.append(str(site + 1))\n",
    "                    ER_df_dict[site + 1] = {\"BUSTEDS-MH ER\": val}\n",
    "                #end if\n",
    "            #end for\n",
    "            # add assert that there are more than 0 sites here.\n",
    "            df_dict[basename].update({\"BUSTEDS-MH_num_ER_Sites\":  len(ER_df_dict.keys())})\n",
    "            x = ER_df_dict.keys()\n",
    "            x = [str(x) for x in x]\n",
    "            df_dict[basename].update({\"BUSTEDS-MH_ER_Sites\":  \"|\".join(x)})\n",
    "            #print(ER_df_dict.keys())\n",
    "        #end if \n",
    "    # end for\n",
    "\n",
    "    df_MH = pd.DataFrame.from_dict(df_dict, orient=\"index\")\n",
    "    df_MH = df_MH.reset_index()\n",
    "    df_MH.index += 1\n",
    "    df_MH.rename(columns={'index': 'Gene'}, inplace = True)\n",
    "    df_MH[\"Dataset\"] = tag\n",
    "    return df_MH, FILES_WITH_ERRORS\n",
    "#end method\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37277f20",
   "metadata": {},
   "source": [
    "## Look over BUSTED[S] Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b543d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_BUSTEDS_results(BUSTEDS_DIR_FILES, tag, FILES_WITH_ERRORS):\n",
    "    df_dict = {}\n",
    "    for item in tqdm(BUSTEDS_DIR_FILES):\n",
    "        basename = os.path.basename(item).replace(\".nex.BUSTEDS.json\", \"\")\n",
    "        # Read json\n",
    "        \n",
    "        json_data_BUSTEDS = read_json(item)\n",
    "        #print(\"# Data loaded:\", item)\n",
    "\n",
    "        if json_data_BUSTEDS == []: \n",
    "            FILES_WITH_ERRORS.append(item)\n",
    "            continue\n",
    "        #end if\n",
    "        \n",
    "        df_dict[basename] = {\"Method\": \"BUSTEDS\"}\n",
    "        df_dict[basename].update({\"Sequences\": json_data_BUSTEDS[\"input\"][\"number of sequences\"]})\n",
    "        df_dict[basename].update({\"Codons\": json_data_BUSTEDS[\"input\"][\"number of sites\"]})\n",
    "        df_dict[basename].update({\"LRT p-value\": json_data_BUSTEDS[\"test results\"][\"p-value\"]})\n",
    "\n",
    "        # cAIC\n",
    "        df_dict[basename].update({\"cAIC\": json_data_BUSTEDS[\"fits\"][\"Unconstrained model\"][\"AIC-c\"]})\n",
    "\n",
    "        A = json_data_BUSTEDS[\"fits\"][\"Unconstrained model\"][\"Rate Distributions\"][\"Test\"][\"0\"][\"omega\"] \n",
    "        B = json_data_BUSTEDS[\"fits\"][\"Unconstrained model\"][\"Rate Distributions\"][\"Test\"][\"1\"][\"omega\"] \n",
    "        C = json_data_BUSTEDS[\"fits\"][\"Unconstrained model\"][\"Rate Distributions\"][\"Test\"][\"2\"][\"omega\"] \n",
    "        df_dict[basename].update({\"CV(omega)\": cv([A, B, C])})\n",
    "\n",
    "        D = json_data_BUSTEDS[\"fits\"][\"Unconstrained model\"][\"Rate Distributions\"][\"Synonymous site-to-site rates\"][\"0\"][\"rate\"] \n",
    "        E = json_data_BUSTEDS[\"fits\"][\"Unconstrained model\"][\"Rate Distributions\"][\"Synonymous site-to-site rates\"][\"1\"][\"rate\"] \n",
    "        F = json_data_BUSTEDS[\"fits\"][\"Unconstrained model\"][\"Rate Distributions\"][\"Synonymous site-to-site rates\"][\"2\"][\"rate\"] \n",
    "        df_dict[basename].update({\"CV(alpha)\": cv([D, E, F])})\n",
    "\n",
    "        #Omegas and proportions\n",
    "        data = json_data_BUSTEDS[\"fits\"][\"Unconstrained model\"][\"Rate Distributions\"][\"Test\"]\n",
    "        w1 = round(data[\"0\"][\"omega\"], 4)\n",
    "        p1 = round(data[\"0\"][\"proportion\"], 4)\n",
    "        w2 = round(data[\"1\"][\"omega\"], 4)\n",
    "        p2 = round(data[\"1\"][\"proportion\"], 4)\n",
    "        w3 = round(data[\"2\"][\"omega\"], 4)\n",
    "        p3 = round(data[\"2\"][\"proportion\"], 4)\n",
    "        df_dict[basename].update({\"w1\": w1, \"p1\": p1})\n",
    "        df_dict[basename].update({\"w2\": w2, \"p2\": p2})\n",
    "        df_dict[basename].update({\"w3\": w3, \"p3\": p3})\n",
    "\n",
    "        # SRV rates and proportions\n",
    "        data = json_data_BUSTEDS[\"fits\"][\"Unconstrained model\"][\"Rate Distributions\"][\"Synonymous site-to-site rates\"]\n",
    "        s1 = round(data[\"0\"][\"rate\"], 4)\n",
    "        s_p1 = round(data[\"0\"][\"proportion\"], 4)\n",
    "        s2 = round(data[\"1\"][\"rate\"], 4)\n",
    "        s_p2 = round(data[\"1\"][\"proportion\"], 4)\n",
    "        s3 = round(data[\"2\"][\"rate\"], 4)\n",
    "        s_p3 = round(data[\"2\"][\"proportion\"], 4)\n",
    "        df_dict[basename].update({\"SRV1\": s1, \"SRV_p1\": s_p1})\n",
    "        df_dict[basename].update({\"SRV2\": s2, \"SRV_p2\": s_p2})\n",
    "        df_dict[basename].update({\"SRV3\": s3, \"SRV_p3\": s3})\n",
    "\n",
    "        # ER Sites\n",
    "        ER_SITES = []\n",
    "        ER_df_dict = {}\n",
    "\n",
    "        if \"constrained\" in json_data_BUSTEDS[\"Evidence Ratios\"].keys():\n",
    "            #print(\"# ER Constrained Sites:\", len(json_data_BUSTEDS[\"Evidence Ratios\"][\"constrained\"][0]))\n",
    "            for site, val in enumerate(json_data_BUSTEDS[\"Evidence Ratios\"][\"constrained\"][0]):\n",
    "                if val > ER_Threshold:\n",
    "                    ER_SITES.append(str(site + 1))\n",
    "                    ER_df_dict[site + 1] = {\"BUSTEDS ER\": val}\n",
    "                #end if\n",
    "            #end for\n",
    "            #df_dict[basename].update({\"num_ER_Sites\":  int(len(ER_df_dict.keys()))})\n",
    "            df_dict[basename].update({\"BUSTEDS_num_ER_Sites\":  len(ER_df_dict.keys())})\n",
    "            x = ER_df_dict.keys()\n",
    "            x = [str(x) for x in x]\n",
    "            df_dict[basename].update({\"BUSTEDS_ER_Sites\":  \"|\".join(x)})\n",
    "            #print(ER_df_dict.keys())\n",
    "        #end if   \n",
    "\n",
    "\n",
    "    # End for\n",
    "\n",
    "    df = pd.DataFrame.from_dict(df_dict, orient=\"index\")\n",
    "    df = df.reset_index()\n",
    "    df.index += 1\n",
    "    df.rename(columns={'index': 'Gene'}, inplace = True)\n",
    "    df[\"Dataset\"] = tag\n",
    "    return df, FILES_WITH_ERRORS\n",
    "#end method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-church",
   "metadata": {},
   "source": [
    "## Calculate cAIC statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc27f9d-a0b7-4798-8879-b142a16e3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(df, df_MH):\n",
    "    df[\"ΔcAIC\"] = \"\"\n",
    "    df[\"RelativeSupport\"] = \"\"\n",
    "    df[\"ER_Sites_Intersection\"] = \"\"\n",
    "    for index, row in tqdm(df_MH.iterrows()):\n",
    "        gene = row[\"Gene\"]\n",
    "        MH_cAIC = float(row[\"cAIC\"])\n",
    "\n",
    "        BUSTEDS_row = df[df[\"Gene\"] == row[\"Gene\"]]\n",
    "        index_BUSTEDS = df[df[\"Gene\"] == row[\"Gene\"]].index\n",
    "\n",
    "        try:\n",
    "            BUSTEDS_cAIC = float(BUSTEDS_row[\"cAIC\"])\n",
    "        except:\n",
    "            #print(BUSTEDS_row)\n",
    "            pass\n",
    "        #end try\n",
    "\n",
    "        BUSTEDSMH_w1, BUSTEDSMH_w2, BUSTEDSMH_w3 = float(row[\"w1\"]), float(row[\"w2\"]), float(row[\"w3\"])\n",
    "        try:\n",
    "            BUSTEDS_w1, BUSTEDS_w2, BUSTEDS_w3 = float(BUSTEDS_row[\"w1\"]), float(BUSTEDS_row[\"w2\"]), float(BUSTEDS_row[\"w3\"])\n",
    "        except:\n",
    "            #print(BUSTEDS_row[\"w1\"], BUSTEDS_row[\"w2\"], BUSTEDS_row[\"w3\"])\n",
    "            continue\n",
    "        #end try\n",
    "\n",
    "        BUSTEDSMH_p1, BUSTEDSMH_p2, BUSTEDSMH_p3 = float(row[\"p1\"]), float(row[\"p2\"]), float(row[\"p3\"])\n",
    "        BUSTEDS_p1, BUSTEDS_p2, BUSTEDS_p3 = float(BUSTEDS_row[\"p1\"]), float(BUSTEDS_row[\"p2\"]), float(BUSTEDS_row[\"p3\"])\n",
    "\n",
    "        BUSTEDSMH_s1, BUSTEDSMH_s2, BUSTEDSMH_s3 = float(row[\"SRV1\"]), float(row[\"SRV2\"]), float(row[\"SRV3\"])\n",
    "        BUSTEDS_s1, BUSTEDS_s2, BUSTEDS_s3 = float(BUSTEDS_row[\"SRV1\"]), float(BUSTEDS_row[\"SRV2\"]), float(BUSTEDS_row[\"SRV3\"])\n",
    "\n",
    "        BUSTEDSMH_sp1, BUSTEDSMH_sp2, BUSTEDSMH_sp3 = float(row[\"SRV_p1\"]), float(row[\"SRV_p2\"]), float(row[\"SRV_p3\"])\n",
    "        BUSTEDS_sp1, BUSTEDS_sp2, BUSTEDS_sp3 = float(BUSTEDS_row[\"SRV_p1\"]), float(BUSTEDS_row[\"SRV_p2\"]), float(BUSTEDS_row[\"SRV_p3\"])\n",
    "\n",
    "        #print(float(BUSTEDS_cAIC[\"cAIC\"]))\n",
    "        best_model = min(MH_cAIC, BUSTEDS_cAIC)\n",
    "        #print()\n",
    "        #print(\"# Gene:\", row[\"Gene\"])\n",
    "\n",
    "        if BUSTEDS_cAIC == best_model:\n",
    "            which_is_best = \"BUSTEDS\"\n",
    "            delta_cAIC = MH_cAIC - best_model\n",
    "            relative_support = math.exp(-delta_cAIC/2)\n",
    "            # add to table\n",
    "            #df.at['C', 'x'] = 10\n",
    "            # \n",
    "            df.at[index_BUSTEDS, \"ΔcAIC\"] = delta_cAIC\n",
    "            df.at[index_BUSTEDS, \"RelativeSupport\"] = relative_support\n",
    "        elif MH_cAIC == best_model:\n",
    "            which_is_best = \"BUSTEDS-MH\"\n",
    "            delta_cAIC = BUSTEDS_cAIC - best_model\n",
    "            relative_support = math.exp(-delta_cAIC/2)\n",
    "            df_MH.at[index, \"ΔcAIC\"] = delta_cAIC\n",
    "            df_MH.at[index, \"RelativeSupport\"] = relative_support\n",
    "        else:\n",
    "            pass\n",
    "        #end if\n",
    "        #print(\"# Best model is:\", best_model, which_is_best, \"by\", delta_cAIC)\n",
    "        #print(\"# With relative support:\", relative_support)\n",
    "\n",
    "        # Intersections of ER Sites.\n",
    "        #print(\"# Examining ER Sites\")\n",
    "        # BUSTEDS-MH_ER_Sites\n",
    "        # BUSTEDS_ER_Sites\n",
    "        try:\n",
    "            BUSTEDS_MH_ER_Sites = row[\"BUSTEDS-MH_ER_Sites\"].split(\"|\")\n",
    "            BUSTEDS_df = df[df[\"Gene\"] == row[\"Gene\"]]\n",
    "            BUSTEDS_ER_Sites    = BUSTEDS_df[\"BUSTEDS_ER_Sites\"].tolist()[0].split(\"|\")\n",
    "            #print(BUSTEDS_MH_ER_Sites, BUSTEDS_ER_Sites)\n",
    "            intersection = set(BUSTEDS_MH_ER_Sites).intersection(BUSTEDS_ER_Sites)\n",
    "            #print(intersection)\n",
    "            df.at[index, \"ER_Sites_Intersection\"] = \"|\".join(intersection)\n",
    "        except:\n",
    "            #print(\"ERROR --\", row[\"BUSTEDS-MH_ER_Sites\"])\n",
    "            pass\n",
    "        #end try\n",
    "\n",
    "\n",
    "        # Wasserstein Distance (Earth Movers)\n",
    "        #print(\"Omegas:\", BUSTEDSMH_w1, BUSTEDSMH_w2, BUSTEDSMH_w3, BUSTEDS_w1, BUSTEDS_w2, BUSTEDS_w3)\n",
    "        WD_unweighted_omega = wasserstein_distance([BUSTEDSMH_w1, BUSTEDSMH_w2, BUSTEDSMH_w3],\n",
    "                                                   [BUSTEDS_w1, BUSTEDS_w2, BUSTEDS_w3])\n",
    "\n",
    "        WD_weighted_omega   = wasserstein_distance([BUSTEDSMH_w1, BUSTEDSMH_w2, BUSTEDSMH_w3], \n",
    "                                                   [BUSTEDS_w1, BUSTEDS_w2, BUSTEDS_w3],\n",
    "                                                   [BUSTEDSMH_p1, BUSTEDSMH_p2, BUSTEDSMH_p3],\n",
    "                                                   [BUSTEDS_p1, BUSTEDS_p2, BUSTEDS_p3])\n",
    "\n",
    "        #print(\"SRV:\", BUSTEDSMH_s1, BUSTEDSMH_s2, BUSTEDSMH_s3, BUSTEDS_s1, BUSTEDS_s2, BUSTEDS_s3)\n",
    "\n",
    "        WD_unweighted_srv   = wasserstein_distance([BUSTEDSMH_s1, BUSTEDSMH_s2, BUSTEDSMH_s3], \n",
    "                                                   [BUSTEDS_s1, BUSTEDS_s2, BUSTEDS_s3])\n",
    "\n",
    "        WD_weighted_srv     = wasserstein_distance([BUSTEDSMH_s1, BUSTEDSMH_s2, BUSTEDSMH_s3], \n",
    "                                                   [BUSTEDS_s1, BUSTEDS_s2, BUSTEDS_s3], \n",
    "                                                   [BUSTEDSMH_sp1, BUSTEDSMH_sp2, BUSTEDSMH_sp3], \n",
    "                                                   [BUSTEDS_sp1, BUSTEDS_sp2, BUSTEDS_sp3])\n",
    "\n",
    "        df_MH.at[index, \"WD_unweighted(omega)\"] = WD_unweighted_omega\n",
    "        df_MH.at[index, \"WD_weighted(omega)\"] = WD_weighted_omega\n",
    "        df_MH.at[index, \"WD_unweighted(srv)\"] = WD_unweighted_srv\n",
    "        df_MH.at[index, \"WD_weighted(srv)\"] = WD_weighted_srv\n",
    "    #end for\n",
    "    return df, df_MH\n",
    "#end method\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5509d43",
   "metadata": {},
   "source": [
    "## Concat tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "908b0296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def concat_tables(df, df_MH):\n",
    "    result = \"\"\n",
    "    result = pd.concat([df_MH, df])\n",
    "    result = result.fillna(\"\")\n",
    "    result = result.sort_values(by=[\"Gene\", \"Method\"], ascending=True)\n",
    "    result = result.reset_index(drop=True)\n",
    "    result.index += 1\n",
    "    return result\n",
    "#end method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-pattern",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-lambda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ================================================================================\n",
      "# Processing files in: Empirical_14_datasets\n",
      "# Number of BUSTEDS results: 14\n",
      "# Number of BUSTEDS-MH results: 14\n",
      "# Processing BUSTED[S] results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:01<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Processing BUSTED[S]-MH results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 368.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Calculating pairwise statistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:00, 933.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Formatting output\n",
      "# Saving results to: E:\\BUSTEDS-MH-develop\\tables\\Table_EMPIRICAL_14_DATASETS.csv\n",
      "# Error log...\n",
      "0 files had errors\n",
      "# ================================================================================\n",
      "\n",
      "# ================================================================================\n",
      "# Processing files in: Empirical_Unmasked_Selectome_v6\n",
      "# Number of BUSTEDS results: 13312\n",
      "# Number of BUSTEDS-MH results: 13312\n",
      "# Processing BUSTED[S] results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|███████████████████████████████████▌                                        | 6238/13312 [00:11<00:15, 461.47it/s]"
     ]
    }
   ],
   "source": [
    "for tag in tags:\n",
    "    print(\"#\", \"=\"*80)\n",
    "    print(\"# Processing files in:\", tag)\n",
    "    \n",
    "    # Additional declares\n",
    "    BUSTEDS_DIR = os.path.join(WD, \"analysis\", tag, \"BUSTEDS\")\n",
    "    BUSTEDS_MH_DIR = os.path.join(WD, \"analysis\", tag, \"BUSTEDS-MH\")\n",
    "    OUTPUT_CSV = os.path.join(WD, \"tables\", \"Table_\" + tag.upper() + \".csv\")\n",
    "    \n",
    "    # Get list of files\n",
    "    BUSTEDS_DIR_FILES = [os.path.join(BUSTEDS_DIR, file.name) for file in os.scandir(BUSTEDS_DIR) if file.name.endswith(\".json\")]\n",
    "    BUSTEDS_MH_DIR_FILES = [os.path.join(BUSTEDS_MH_DIR, file.name) for file in os.scandir(BUSTEDS_MH_DIR) if file.name.endswith(\".json\")]\n",
    "\n",
    "    # Report to user\n",
    "    print(\"# Number of BUSTEDS results:\", len(BUSTEDS_DIR_FILES))\n",
    "    print(\"# Number of BUSTEDS-MH results:\", len(BUSTEDS_MH_DIR_FILES))\n",
    "    \n",
    "    # Process files separately\n",
    "    print(\"# Processing BUSTED[S] results\")\n",
    "    df, FILES_WITH_ERRORS    = get_BUSTEDS_results(BUSTEDS_DIR_FILES, tag, FILES_WITH_ERRORS)\n",
    "    print(\"# Processing BUSTED[S]-MH results\")\n",
    "    df_MH, FILES_WITH_ERRORS = get_BUSTEDS_MH_results(BUSTEDS_MH_DIR_FILES, tag, FILES_WITH_ERRORS)\n",
    "    \n",
    "    # Calculate pairwise statistics\n",
    "    print(\"# Calculating pairwise statistics\")\n",
    "    df, df_MH = calculate_statistics(df, df_MH)\n",
    "    \n",
    "    # Combine results\n",
    "    print(\"# Formatting output\")\n",
    "    results = concat_tables(df, df_MH)\n",
    "    \n",
    "    # Save to file\n",
    "    print(\"# Saving results to:\", OUTPUT_CSV)\n",
    "    results.to_csv(OUTPUT_CSV, index=False)\n",
    "    \n",
    "    # Files with errors\n",
    "    print(\"# Error log...\")\n",
    "    print(len(FILES_WITH_ERRORS), \"files had errors\")\n",
    "\n",
    "    with open(\"ERRORS_\" + tag + \".log\", \"w\") as fh:\n",
    "        for file in FILES_WITH_ERRORS:\n",
    "            print(file, file=fh)\n",
    "        #end for\n",
    "    #end with\n",
    "    print(\"#\", \"=\"*80)\n",
    "    print()\n",
    "#end for\n",
    "\n",
    "print(\"# Done processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4583c5bb",
   "metadata": {},
   "source": [
    "## End of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note Negative delta LL are convergence problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower AIC values indicate a better-fit model, and a model with a delta-AIC (the difference between the two AIC values being compared) of more than -2 is considered significantly better than the model it is being compared to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earth Mover's (Kantorovich) distance between two distrbuitions if you want a single number"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
